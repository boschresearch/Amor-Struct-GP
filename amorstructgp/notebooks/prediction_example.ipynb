{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction example of trained amortization network\n",
    "\n",
    "This is a minimal example how to use our amortization network for GP inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amorstructgp.models.gp_model_amortized_structured import GPModelAmortizedStructured\n",
    "from amorstructgp.config.models.gp_model_amortized_structured_config import PaperAmortizedStructuredConfig,AmortizedStructuredWithMaternConfig\n",
    "from amorstructgp.models.model_factory import ModelFactory\n",
    "from amorstructgp.gp.base_symbols import BaseKernelTypes\n",
    "from amorstructgp.data_generators.simulator import Simulator\n",
    "from amorstructgp.config.prior_parameters import NOISE_VARIANCE_EXPONENTIAL_LAMBDA\n",
    "from amorstructgp.gp.base_kernels import transform_kernel_list_to_expression\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint Paths \n",
    "\n",
    "We uploaded two pretrained weights - the one used in the paper (with base kernels SE,PER and LIN and their two-gram multiplications) and one with the Matern-52 kernel additionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PAPER_MODEL = False\n",
    "PATH_TO_PRETRAINED=\"\"\n",
    "paper_model_path = os.path.join(PATH_TO_PRETRAINED,\"main_state_dict_paper.pth\")\n",
    "matern_model_path = os.path.join(PATH_TO_PRETRAINED,\"main_state_dict_with_matern.pth\")\n",
    "if USE_PAPER_MODEL:\n",
    "    model_path = paper_model_path\n",
    "else:\n",
    "    model_path = matern_model_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "We use the factory pattern to build models that admit to the `BaseModel` interface. The `BaseModel` child class assiciated with the amortization networks is `GPModelAmortizedStructured` and bascically forms wrapper around the actual torch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PAPER_MODEL:\n",
    "    amortized_model_config = PaperAmortizedStructuredConfig(checkpoint_path=model_path)\n",
    "else:\n",
    "    amortized_model_config = AmortizedStructuredWithMaternConfig(checkpoint_path=model_path)\n",
    "model = ModelFactory.build(amortized_model_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify input kernel\n",
    "\n",
    "Input kernels are configured as nested lists for example `[[BaseKernelTypes.SE,BaseKernelTypes.LIN]]` where the elements of the outer list describe the kernels in the seperate dimensions (that are multiplied) and the elements of the inner list describe the base kernels inside each dimension - the list in this case is interpreted as an addition of the base kernels. The possible base kernel can be deduced from the `BaseKernelTypes` enums (the paper model is not trained on `BaseKernelTypes` that involve Matern52 kernels).\n",
    "\n",
    "Example kernels:\n",
    "\n",
    "1D data:\n",
    "- Linear + SE kernel: `[[BaseKernelTypes.LIN,BaseKernelTypes.SE]]`\n",
    "- Linear + SE x PER kernel:  `[[BaseKernelTypes.LIN,BaseKernelTypes.SE_MULT_PER]]`\n",
    "\n",
    "2D data:\n",
    "- SE1 x SE2 (RBF kernel): `[[BaseKernelTypes.SE],[BaseKernelTypes.SE]]`\n",
    "- (LIN1 + SE1) x SE2: `[[BaseKernelTypes.LIN,BaseKernelTypes.SE],[BaseKernelTypes.SE]]`\n",
    "\n",
    "In case the dataset is multidimensional and the outer list has only one element e.g. `[[BaseKernelTypes.LIN,BaseKernelTypes.SE]]` - this is interpreted by the `GPModelAmortizedStructured` object as applying the same kernel structure to each dimension in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_kernel = [[BaseKernelTypes.SE,BaseKernelTypes.LIN]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate simulated dataset\n",
    "\n",
    "Here we create an example dataset - generated from a GP with ground truth kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_kernel = [[BaseKernelTypes.MATERN52,BaseKernelTypes.LIN]] \n",
    "\n",
    "########### Dataset configurations ###########\n",
    "\n",
    "# range from which gt data is generated - input data of amortization should lie between 0.0 and 1.0 but test data can lie outside\n",
    "data_a = -0.5\n",
    "data_b = 1.5 \n",
    "\n",
    "n_data = 10 # number of training datapoint - in (0.0,1.0)\n",
    "n_test = 400 # number of test datapoints - uniform random in (data_a,data_b)\n",
    "observation_noise = 0.05 # obersation noise that is added to the data\n",
    "\n",
    "n_sim = 500 # number of simulated points in (data_a,data_b) - can be set to a high number - n_data is drawn from this set\n",
    "simulator = Simulator(data_a,data_b,NOISE_VARIANCE_EXPONENTIAL_LAMBDA)\n",
    "kernel_expression = transform_kernel_list_to_expression(simulator_kernel)\n",
    "simulated_dataset = simulator.create_sample(n_sim,n_test,kernel_expression,observation_noise)\n",
    "simulated_dataset.add_kernel_list_gt(simulator_kernel)\n",
    "input_dimension = simulated_dataset.get_input_dimension()\n",
    "x_data,y_data = simulated_dataset.get_dataset()\n",
    "y_data =np.expand_dims(y_data[(x_data >=0.0) & (x_data<=1.0)],axis=1)[:n_data]\n",
    "x_data =np.expand_dims(x_data[(x_data >=0.0) & (x_data<=1.0)],axis=1)[:n_data]\n",
    "x_test,y_test = simulated_dataset.get_test_dataset()\n",
    "_,f_test = simulated_dataset.get_ground_truth_f()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make inference and prediction\n",
    "\n",
    "We use the `BaseModel` function to do inference and prediction. First we set the input kernel in the `GPModelAmortizedStructured` object via `.set_kernel_list(input_kernel)`. This configures the kernel inside the amortization network. When calling `infer(x_data,y_data)` on an `GPModelAmortizedStructured` a forward pass through the amortization network is done to predict all hyperparameters - this also caches the dataset for prediction. When calling `predictive_dist(x_test)`, we evaluated the predictive distribtion of the GP with the predicted hyperparameter and get predictive mus and sigmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from amorstructgp.utils.plotter import PlotterPlotly\n",
    "\n",
    "\n",
    "model.set_kernel_list(input_kernel)\n",
    "kernel_parameters,noise_variances =model.get_predicted_parameters(x_data,y_data)\n",
    "model.infer(x_data,y_data)\n",
    "pred_mu,pred_sigma = model.predictive_dist(x_test)\n",
    "nll_model = -1*np.mean(model.predictive_log_likelihood(x_test,y_test))\n",
    "\n",
    "plotter = PlotterPlotly(1)\n",
    "plotter.add_gt_function(x_test,f_test,\"red\",0)\n",
    "plotter.add_predictive_dist(np.squeeze(x_test),np.squeeze(pred_mu),np.squeeze(pred_sigma),0)\n",
    "plotter.add_datapoints(x_data,y_data,\"limegreen\",0)\n",
    "plotter.show()\n",
    "\n",
    "display(\"Predicted parameters:\")\n",
    "display(kernel_parameters)\n",
    "display(torch.sqrt(noise_variances))\n",
    "display(\"GT parameters:\")\n",
    "display(simulated_dataset.get_gt_kernel_parameter_list())\n",
    "display(simulated_dataset.get_observation_noise())\n",
    "display(f\"NLL amor model: {nll_model}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9609984879b405a788741d92268f3aba3ee6f607f73ee694a640bfa1ba953bc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
